#!/usr/bin/env python3
"""
é«˜æ€§èƒ½å¹¶è¡ŒåŒ–è½¦ç‰Œè®°å½•åˆ é™¤å·¥å…·
æ”¯æŒæ‰¹é‡åˆ é™¤XLSXæ–‡ä»¶ä¸­çš„è½¦ç‰Œè®°å½•ï¼Œå…·å¤‡æ™ºèƒ½è´Ÿè½½å‡è¡¡å’Œå®æ—¶ç›‘æ§åŠŸèƒ½
"""

import os
import sys
import argparse
import pandas as pd
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime
import gc
import psutil
from typing import List, Set, Tuple, Optional, Dict
import time
from dataclasses import dataclass
from functools import partial
import logging


@dataclass
class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_files: int = 0
    processed_files: int = 0
    failed_files: int = 0
    total_records_deleted: int = 0
    start_time: float = 0
    
    @property
    def success_rate(self) -> float:
        if self.total_files == 0:
            return 0.0
        return (self.processed_files / self.total_files) * 100
    
    @property
    def processing_speed(self) -> float:
        elapsed = time.time() - self.start_time
        if elapsed == 0:
            return 0.0
        return self.processed_files / elapsed


class VehicleDataProcessor:
    """æ ¸å¿ƒæ•°æ®å¤„ç†æ¨¡å—"""
    
    def __init__(self, vehicle_plates: Set[str], from_date: Optional[datetime] = None):
        self.vehicle_plates = vehicle_plates
        self.from_date = from_date
        
    def extract_date_from_filename(self, filename: str) -> Optional[datetime]:
        """ä»æ–‡ä»¶åæå–æ—¥æœŸ (e.g., '0101.xlsx' -> datetime(2025, 1, 1))"""
        try:
            stem = Path(filename).stem
            if len(stem) == 4 and stem.isdigit():
                month = int(stem[:2])
                day = int(stem[2:])
                # é»˜è®¤ä½¿ç”¨å½“å‰å¹´ä»½
                year = datetime.now().year
                return datetime(year, month, day)
        except (ValueError, IndexError):
            pass
        return None
    
    def extract_date_from_content(self, df: pd.DataFrame) -> Optional[datetime]:
        """ä»æ•°æ®å†…å®¹ä¸­æå–æ—¥æœŸ"""
        for col in df.columns:
            if any(keyword in str(col).lower() for keyword in ['æ—¶é—´', 'date', 'æ—¥æœŸ']):
                try:
                    # æŸ¥æ‰¾ç‰¹æ®Šæ ¼å¼: "2025/1/1 0:00:00---2025/1/1 0:00:00åˆè®¡:"
                    for value in df[col].dropna().head(10):
                        value_str = str(value)
                        if '---' in value_str and 'åˆè®¡' in value_str:
                            date_part = value_str.split('---')[0].strip()
                            return datetime.strptime(date_part.split()[0], '%Y/%m/%d')
                except (ValueError, AttributeError):
                    continue
        return None
    
    def should_process_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤„ç†è¯¥æ–‡ä»¶ï¼ˆåŸºäºæ—¥æœŸè¿‡æ»¤ï¼‰"""
        if not self.from_date:
            return True
            
        # å…ˆå°è¯•ä»æ–‡ä»¶åæå–æ—¥æœŸ
        file_date = self.extract_date_from_filename(file_path)
        if file_date:
            return file_date >= self.from_date
            
        # å¦‚æœæ–‡ä»¶åæ— æ³•æå–æ—¥æœŸï¼Œåˆ™éœ€è¦æ‰“å¼€æ–‡ä»¶æ£€æŸ¥å†…å®¹
        return True  # å»¶è¿Ÿåˆ°å†…å®¹æ£€æŸ¥æ—¶åˆ¤æ–­
    
    def process_single_file(self, file_path: str) -> Tuple[bool, int, str]:
        """å¤„ç†å•ä¸ªXLSXæ–‡ä»¶"""
        try:
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(file_path, engine='openpyxl')
            
            if df.empty:
                return True, 0, "ç©ºæ–‡ä»¶"
            
            # å¦‚æœæœ‰æ—¥æœŸè¿‡æ»¤ä¸”æ–‡ä»¶åæ— æ³•ç¡®å®šæ—¥æœŸï¼Œæ£€æŸ¥æ–‡ä»¶å†…å®¹
            if self.from_date and not self.extract_date_from_filename(file_path):
                content_date = self.extract_date_from_content(df)
                if content_date and content_date < self.from_date:
                    return True, 0, "æ—¥æœŸä¸ç¬¦åˆè¿‡æ»¤æ¡ä»¶"
            
            # æŸ¥æ‰¾è½¦ç‰Œå·åˆ—
            plate_column = None
            for col in df.columns:
                if any(keyword in str(col).lower() for keyword in ['è½¦ç‰Œ', 'plate', 'å·ç‰Œ']):
                    plate_column = col
                    break
            
            if plate_column is None:
                return False, 0, "æœªæ‰¾åˆ°è½¦ç‰Œå·åˆ—"
            
            # åˆ é™¤åŒ¹é…çš„è½¦ç‰Œè®°å½•
            original_count = len(df)
            df_filtered = df[~df[plate_column].astype(str).isin(self.vehicle_plates)]
            deleted_count = original_count - len(df_filtered)
            
            if deleted_count > 0:
                # ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶
                df_filtered.to_excel(file_path, index=False, engine='openpyxl')
            
            return True, deleted_count, f"æˆåŠŸåˆ é™¤ {deleted_count} æ¡è®°å½•"
            
        except Exception as e:
            return False, 0, f"å¤„ç†å¤±è´¥: {str(e)}"


def process_file_batch(file_paths: List[str], vehicle_plates: Set[str], 
                      from_date: Optional[datetime] = None) -> Dict:
    """æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆå­è¿›ç¨‹æ‰§è¡Œï¼‰"""
    processor = VehicleDataProcessor(vehicle_plates, from_date)
    results = {
        'processed': 0,
        'failed': 0,
        'total_deleted': 0,
        'errors': []
    }
    
    for file_path in file_paths:
        if not processor.should_process_file(file_path):
            continue
            
        success, deleted_count, message = processor.process_single_file(file_path)
        
        if success:
            results['processed'] += 1
            results['total_deleted'] += deleted_count
        else:
            results['failed'] += 1
            results['errors'].append(f"{file_path}: {message}")
    
    return results


class ParallelVehicleRemover:
    """å¹¶è¡ŒåŒ–è½¦ç‰Œè®°å½•åˆ é™¤å™¨"""
    
    def __init__(self, max_workers: Optional[int] = None, memory_limit_gb: float = 2.0):
        self.max_workers = max_workers or min(mp.cpu_count(), 8)
        self.memory_limit_bytes = memory_limit_gb * 1024 * 1024 * 1024
        self.stats = ProcessingStats()
        
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨ç‡"""
        process = psutil.Process()
        return process.memory_info().rss
    
    def _balance_workload(self, xlsx_files: List[str]) -> List[List[str]]:
        """æ™ºèƒ½è´Ÿè½½å‡è¡¡ - æ ¹æ®æ–‡ä»¶å¤§å°åˆ†é…ä»»åŠ¡"""
        if not xlsx_files:
            return []
            
        # è·å–æ–‡ä»¶å¤§å°
        file_weights = []
        for file_path in xlsx_files:
            try:
                size = os.path.getsize(file_path)
                file_weights.append((file_path, size))
            except OSError:
                # æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ï¼Œè·³è¿‡
                continue
        
        if not file_weights:
            return []
        
        # æŒ‰æ–‡ä»¶å¤§å°é™åºæ’åˆ—
        file_weights.sort(key=lambda x: x[1], reverse=True)
        
        # è´ªå¿ƒç®—æ³•åˆ†é…åˆ°å„ä¸ªworker
        workers = [[] for _ in range(self.max_workers)]
        worker_loads = [0] * self.max_workers
        
        for file_path, size in file_weights:
            # åˆ†é…ç»™å½“å‰è´Ÿè½½æœ€å°çš„worker
            min_idx = worker_loads.index(min(worker_loads))
            workers[min_idx].append(file_path)
            worker_loads[min_idx] += size
        
        # è¿‡æ»¤æ‰ç©ºçš„workerç»„
        return [worker for worker in workers if worker]
    
    def _monitor_progress(self, futures: List, total_files: int) -> ProcessingStats:
        """å®æ—¶ç›‘æ§å¤„ç†è¿›åº¦"""
        self.stats.total_files = total_files
        self.stats.start_time = time.time()
        
        print(f"ğŸš€ å¹¶è¡Œå¤„ç†å¼€å§‹ [Workers: {len(futures)}, Files: {total_files}]")
        
        for future in as_completed(futures):
            try:
                result = future.result()
                self.stats.processed_files += result['processed']
                self.stats.failed_files += result['failed']
                self.stats.total_records_deleted += result['total_deleted']
                
                # è¾“å‡ºè¿›åº¦
                progress = (self.stats.processed_files + self.stats.failed_files) / total_files
                memory_mb = self._get_memory_usage() / (1024 * 1024)
                
                print(f"\rğŸ“Š Progress: {'â–ˆ' * int(progress * 20):<20} "
                      f"{progress * 100:.1f}% ({self.stats.processed_files + self.stats.failed_files}/{total_files}) "
                      f"âš¡ {self.stats.processing_speed:.1f} files/sec "
                      f"ğŸ§  {memory_mb:.0f}MB", end='', flush=True)
                
                # å¤„ç†é”™è¯¯ä¿¡æ¯
                if result['errors']:
                    for error in result['errors']:
                        logging.warning(error)
                        
            except Exception as e:
                logging.error(f"å¤„ç†ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                self.stats.failed_files += 1
        
        print()  # æ¢è¡Œ
        return self.stats
    
    def process_files_parallel(self, xlsx_files: List[str], vehicle_plates: Set[str], 
                             from_date: Optional[datetime] = None) -> ProcessingStats:
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        if not xlsx_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„XLSXæ–‡ä»¶")
            return self.stats
        
        # è´Ÿè½½å‡è¡¡åˆ†ç»„
        file_groups = self._balance_workload(xlsx_files)
        
        if not file_groups:
            print("âŒ æ‰€æœ‰æ–‡ä»¶éƒ½æ— æ³•è®¿é—®")
            return self.stats
        
        # åˆ›å»ºè¿›ç¨‹æ± å¹¶æäº¤ä»»åŠ¡
        with ProcessPoolExecutor(max_workers=len(file_groups)) as executor:
            process_func = partial(process_file_batch, 
                                 vehicle_plates=vehicle_plates, 
                                 from_date=from_date)
            
            futures = [executor.submit(process_func, file_group) 
                      for file_group in file_groups]
            
            # ç›‘æ§è¿›åº¦
            self.stats = self._monitor_progress(futures, len(xlsx_files))
        
        return self.stats


def load_vehicle_plates(csv_file: str) -> Set[str]:
    """ä»CSVæ–‡ä»¶åŠ è½½è½¦ç‰Œå·åˆ—è¡¨"""
    try:
        df = pd.read_csv(csv_file)
        
        # æŸ¥æ‰¾è½¦ç‰Œå·åˆ—
        plate_column = None
        for col in df.columns:
            if any(keyword in str(col).lower() for keyword in ['è½¦ç‰Œ', 'plate', 'å·ç‰Œ']):
                plate_column = col
                break
        
        if plate_column is None:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„è½¦ç‰Œåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—
            plate_column = df.columns[0]
            print(f"âš ï¸  æœªæ‰¾åˆ°æ˜ç¡®çš„è½¦ç‰Œåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—: {plate_column}")
        
        plates = set(df[plate_column].astype(str).str.strip())
        plates.discard('')  # ç§»é™¤ç©ºå­—ç¬¦ä¸²
        plates.discard('nan')  # ç§»é™¤NaNå€¼
        
        print(f"ğŸ“‹ æˆåŠŸåŠ è½½ {len(plates)} ä¸ªè½¦ç‰Œå·")
        return plates
        
    except Exception as e:
        print(f"âŒ åŠ è½½è½¦ç‰Œå·æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)


def find_xlsx_files(data_dir: str) -> List[str]:
    """æŸ¥æ‰¾æ‰€æœ‰XLSXæ–‡ä»¶"""
    data_path = Path(data_dir)
    if not data_path.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return []
    
    xlsx_files = list(data_path.glob("*.xlsx"))
    print(f"ğŸ“ åœ¨ç›®å½• {data_dir} ä¸­æ‰¾åˆ° {len(xlsx_files)} ä¸ªXLSXæ–‡ä»¶")
    
    return [str(f) for f in xlsx_files]


def parse_date(date_str: str) -> datetime:
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
    try:
        return datetime.strptime(date_str, '%Y-%m-%d')
    except ValueError:
        print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {date_str}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="é«˜æ€§èƒ½å¹¶è¡ŒåŒ–è½¦ç‰Œè®°å½•åˆ é™¤å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python remove_vehicle_records.py filter_data/delete_all.csv
  python remove_vehicle_records.py filter_data/from_2025_02_01.csv --from-date 2025-02-01
  python remove_vehicle_records.py filter_data/delete_all.csv --parallel --workers 6
        """
    )
    
    parser.add_argument('csv_file', help='åŒ…å«è¦åˆ é™¤è½¦ç‰Œå·çš„CSVæ–‡ä»¶')
    parser.add_argument('--from-date', type=parse_date, 
                       help='åªåˆ é™¤æŒ‡å®šæ—¥æœŸåŠä»¥åçš„è®°å½• (æ ¼å¼: YYYY-MM-DD)')
    parser.add_argument('--data-dir', default='data', 
                       help='XLSXæ–‡ä»¶æ‰€åœ¨ç›®å½• (é»˜è®¤: data)')
    parser.add_argument('--parallel', action='store_true', 
                       help='å¯ç”¨å¹¶è¡Œå¤„ç†')
    parser.add_argument('--workers', type=int, 
                       help='å¹¶è¡Œworkeræ•°é‡ (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--memory-limit', type=float, default=2.0,
                       help='å†…å­˜é™åˆ¶ (GB, é»˜è®¤: 2.0)')
    parser.add_argument('--verbose', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    else:
        logging.basicConfig(level=logging.WARNING, format='%(levelname)s: %(message)s')
    
    # åŠ è½½è½¦ç‰Œå·
    vehicle_plates = load_vehicle_plates(args.csv_file)
    if not vehicle_plates:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è½¦ç‰Œå·")
        return
    
    # æŸ¥æ‰¾XLSXæ–‡ä»¶
    xlsx_files = find_xlsx_files(args.data_dir)
    if not xlsx_files:
        return
    
    # å¼€å§‹å¤„ç†
    start_time = time.time()
    
    if args.parallel:
        # å¹¶è¡Œå¤„ç†
        remover = ParallelVehicleRemover(
            max_workers=args.workers,
            memory_limit_gb=args.memory_limit
        )
        stats = remover.process_files_parallel(xlsx_files, vehicle_plates, args.from_date)
    else:
        # ä¸²è¡Œå¤„ç†
        processor = VehicleDataProcessor(vehicle_plates, args.from_date)
        stats = ProcessingStats()
        stats.total_files = len(xlsx_files)
        stats.start_time = start_time
        
        for i, file_path in enumerate(xlsx_files, 1):
            if processor.should_process_file(file_path):
                success, deleted_count, message = processor.process_single_file(file_path)
                if success:
                    stats.processed_files += 1
                    stats.total_records_deleted += deleted_count
                else:
                    stats.failed_files += 1
                    logging.warning(f"{file_path}: {message}")
            
            # ç®€å•è¿›åº¦æ˜¾ç¤º
            if i % 10 == 0 or i == len(xlsx_files):
                print(f"å¤„ç†è¿›åº¦: {i}/{len(xlsx_files)}")
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    elapsed_time = time.time() - start_time
    print(f"\n{'='*50}")
    print(f"ğŸ¯ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   â€¢ æ€»æ–‡ä»¶æ•°: {stats.total_files}")
    print(f"   â€¢ æˆåŠŸå¤„ç†: {stats.processed_files}")
    print(f"   â€¢ å¤„ç†å¤±è´¥: {stats.failed_files}")
    print(f"   â€¢ æˆåŠŸç‡: {stats.success_rate:.1f}%")
    print(f"   â€¢ åˆ é™¤è®°å½•æ€»æ•°: {stats.total_records_deleted}")
    print(f"   â€¢ å¤„ç†è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"   â€¢ å¹³å‡é€Ÿåº¦: {stats.processing_speed:.2f} æ–‡ä»¶/ç§’")
    print(f"{'='*50}")


if __name__ == '__main__':
    main()